{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Dataset with expanded Q&A pairs (add as many as possible)\ndata = {\n    \"Question\": [\n        \"What is the name of the company?\",\n        \"Where is Zamu AI based?\",\n        \"What services does Zamu AI provide?\",\n        \"What is the focus area of Zamu AI?\",\n        \"Where is the headquarters of Zamu AI located?\",\n        \"Does Zamu AI offer services related to language models?\",\n        \"Can Zamu AI help with data gathering and training models?\",\n        \"What solutions does Zamu AI provide for AI development?\",\n        \"Is Zamu AI a remote-based company?\",\n        \"What is LangChain, and does Zamu AI work with it?\"\n    ],\n    \"Answer\": [\n        \"The name of the company is Zamu AI.\",\n        \"Zamu AI is a remote-based company.\",\n        \"Zamu AI provides AI-related solutions, data gathering, model training, LLM services, and LangChain solutions.\",\n        \"Zamu AI focuses on providing AI solutions, including data gathering and training models.\",\n        \"The headquarters of Zamu AI is located in Peshawar, Pakistan.\",\n        \"Yes, Zamu AI offers services related to large language models (LLMs) and LangChain solutions.\",\n        \"Yes, Zamu AI assists with data gathering and model training services.\",\n        \"Zamu AI provides a range of AI development solutions, including data gathering, training models, and large language models.\",\n        \"Yes, Zamu AI is a remote-based company.\",\n        \"LangChain is a framework for building applications with large language models, and Zamu AI offers solutions that involve LangChain.\"\n    ]\n}\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_dict(data)\n\n# Load model and tokenizer\nmodel_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Set the padding token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Preprocess the dataset\ndef preprocess_function(examples):\n    inputs = [f\"Question: {q}\\nAnswer: {a}\" for q, a in zip(examples[\"Question\"], examples[\"Answer\"])]\n    model_inputs = tokenizer(inputs, padding=\"max_length\", max_length=128, truncation=True)\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n\n# Fine-tuning settings with lower learning rate and more epochs\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=2,\n    num_train_epochs=10,  # Increase number of epochs\n    learning_rate=5e-5,   # Lower learning rate\n    warmup_steps=100,     # Warmup steps to help with convergence\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\"\n)\n\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Save the model\nmodel.save_pretrained(\"fine_tuned_model\")\ntokenizer.save_pretrained(\"fine_tuned_model\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:51:35.133896Z","iopub.execute_input":"2024-11-03T07:51:35.134595Z","iopub.status.idle":"2024-11-03T07:51:44.156362Z","shell.execute_reply.started":"2024-11-03T07:51:35.134550Z","shell.execute_reply":"2024-11-03T07:51:44.155509Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e8ca0a09f842b6ab6d0a9ed792dd11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:06, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('fine_tuned_model/tokenizer_config.json',\n 'fine_tuned_model/special_tokens_map.json',\n 'fine_tuned_model/vocab.json',\n 'fine_tuned_model/merges.txt',\n 'fine_tuned_model/added_tokens.json',\n 'fine_tuned_model/tokenizer.json')"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport torch\n\n# Define an expanded and varied Q&A dataset with more explicit and detailed answers\ndata = {\n    \"Question\": [\n        \"What is the name of the company?\",\n        \"Where is Zamu AI based?\",\n        \"What services does Zamu AI provide?\",\n        \"What are the main offerings of Zamu AI?\",\n        \"What is the focus area of Zamu AI?\",\n        \"Where is the headquarters of Zamu AI located?\",\n        \"Is Zamu AI a remote-based company?\",\n        \"Does Zamu AI offer services related to large language models?\",\n        \"Can Zamu AI assist with data gathering and training models?\",\n        \"What solutions does Zamu AI provide for AI development?\",\n        \"Who is the target audience for Zamu AI?\",\n        \"What are the primary technologies used by Zamu AI?\",\n        \"What industries does Zamu AI serve?\",\n        \"Can Zamu AI help with digital transformation projects?\",\n        \"What is LangChain, and does Zamu AI work with it?\",\n    ],\n    \"Answer\": [\n        \"The name of the company is Zamu AI.\",\n        \"Zamu AI is based in Peshawar, Pakistan, and operates remotely.\",\n        \"Zamu AI provides services including AI model training, data gathering, LLM solutions, and LangChain integration.\",\n        \"The main offerings of Zamu AI are AI solution development, data gathering, and training models for various applications.\",\n        \"Zamu AI focuses on providing advanced AI solutions, data management, and large language models for clients.\",\n        \"The headquarters of Zamu AI is located in Peshawar, Pakistan.\",\n        \"Yes, Zamu AI is a remote-based company, allowing employees to work from anywhere.\",\n        \"Yes, Zamu AI offers services related to large language models (LLMs) and custom model training.\",\n        \"Yes, Zamu AI helps with data gathering and model training as part of its services.\",\n        \"Zamu AI provides end-to-end solutions for AI development, including data preparation, model training, and deployment.\",\n        \"Zamu AI primarily targets businesses seeking AI solutions for operational improvement.\",\n        \"Zamu AI uses technologies such as NLP, deep learning, and large language models to power its solutions.\",\n        \"Zamu AI serves industries like healthcare, finance, retail, and more, providing AI-driven insights.\",\n        \"Yes, Zamu AI supports digital transformation projects using cutting-edge AI technologies.\",\n        \"LangChain is a framework for large language model applications, and Zamu AI utilizes it in developing custom AI solutions.\",\n    ]\n}\n\n# Convert to Hugging Face Dataset\ndataset = Dataset.from_dict(data)\n\n# Load Flan-T5 model and tokenizer\nmodel_name = \"google/flan-t5-base\"  # Use Flan-T5-base for better instruction-following capabilities\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n# Preprocess the dataset\ndef preprocess_function(examples):\n    inputs = [f\"Question: {q} Answer:\" for q in examples[\"Question\"]]\n    targets = examples[\"Answer\"]\n    model_inputs = tokenizer(inputs, max_length=128, padding=\"max_length\", truncation=True)\n    labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=4,\n    num_train_epochs=10,       # Increase for better learning\n    learning_rate=1e-5,        # Lower learning rate for gradual fine-tuning\n    warmup_steps=100,\n    save_steps=500,\n    save_total_limit=2,\n    report_to=\"none\"  # Disable any external logging\n)\n\n# Set up Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n)\n\n# Fine-tune the model\ntrainer.train()\n\n# Save the fine-tuned model\nmodel.save_pretrained(\"fine_tuned_model_flan_t5\")\ntokenizer.save_pretrained(\"fine_tuned_model_flan_t5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T08:10:35.025871Z","iopub.execute_input":"2024-11-03T08:10:35.026631Z","iopub.status.idle":"2024-11-03T08:11:13.707686Z","shell.execute_reply.started":"2024-11-03T08:10:35.026589Z","shell.execute_reply":"2024-11-03T08:11:13.706675Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82070d2085f24066b044042d13ffbe57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ea0a8bea5e4a25b6d788cd6f737a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"824ff7bfb1b54f5ba8d00357d6fc667a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83eb3c59fd724664942d788ab1a6dbf5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9b5056342a8409f933da5320c4a1f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0363a1f77a0e4584913704f66a062f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36a7d4a270bd4ec59ea07133cafea9a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/15 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"116c11289bef44e6ab363eaed4bf468b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [40/40 00:27, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"('fine_tuned_model_flan_t5/tokenizer_config.json',\n 'fine_tuned_model_flan_t5/special_tokens_map.json',\n 'fine_tuned_model_flan_t5/spiece.model',\n 'fine_tuned_model_flan_t5/added_tokens.json',\n 'fine_tuned_model_flan_t5/tokenizer.json')"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n\n# Load a BERT model fine-tuned on question answering (SQuAD)\nmodel_name = \"deepset/bert-base-cased-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\n\n# Example questions and contexts\nqa_pairs = [\n    {\n        \"question\": \"What services does Zamu AI provide?\",\n        \"context\": \"Zamu AI provides AI-related solutions, data gathering, model training, and large language model (LLM) services. It also offers LangChain solutions.\"\n    },\n    {\n        \"question\": \"Where is Zamu AI based?\",\n        \"context\": \"Zamu AI is based in Peshawar, Pakistan, and operates as a remote-based company.\"\n    },\n    {\n        \"question\": \"Can Zamu AI help with large language models?\",\n        \"context\": \"Yes, Zamu AI provides services related to large language models, including model development, training, and deployment.\"\n    },\n    {\n        \"question\": \"What is LangChain and how is it used in AI?\",\n        \"context\": \"LangChain is a framework for building applications with large language models, often used in NLP tasks. Zamu AI uses LangChain for custom AI solution development.\"\n    }\n]\n\n# Set up the question answering pipeline with GPU acceleration\nqa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)  # Use GPU\n\n# Test the model on queries\nfor qa in qa_pairs:\n    response = qa_pipeline(question=qa[\"question\"], context=qa[\"context\"])\n    print(f\"Query: {qa['question']}\")\n    print(f\"Response: {response['answer']}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T08:13:04.302223Z","iopub.execute_input":"2024-11-03T08:13:04.303122Z","iopub.status.idle":"2024-11-03T08:13:04.857590Z","shell.execute_reply.started":"2024-11-03T08:13:04.303075Z","shell.execute_reply":"2024-11-03T08:13:04.856576Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"Query: What services does Zamu AI provide?\nResponse: data gathering, model training, and large language model (LLM) services\n\nQuery: Where is Zamu AI based?\nResponse: Peshawar, Pakistan\n\nQuery: Can Zamu AI help with large language models?\nResponse: Zamu AI provides services\n\nQuery: What is LangChain and how is it used in AI?\nResponse: a framework for building applications with large language models\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load the fine-tuned T5 model\nmodel_path = \"fine_tuned_model_t5\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n# Test the model with sample queries\nqueries = [\n    \"What services does Zamu AI provide?\",\n    \"Where is Zamu AI based?\",\n    \"Can Zamu AI help with large language models?\",\n    \"What is LangChain and how is it used in AI?\",\n]\n\nfor query in queries:\n    inputs = tokenizer(f\"question: {query} answer:\", return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_length=50)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Query: {query}\")\n    print(f\"Response: {response}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T07:53:36.539952Z","iopub.execute_input":"2024-11-03T07:53:36.540824Z","iopub.status.idle":"2024-11-03T07:53:37.279384Z","shell.execute_reply.started":"2024-11-03T07:53:36.540783Z","shell.execute_reply":"2024-11-03T07:53:37.278437Z"}},"outputs":[{"name":"stdout","text":"Query: What services does Zamu AI provide?\nResponse: a response:\n\nQuery: Where is Zamu AI based?\nResponse: True\n\nQuery: Can Zamu AI help with large language models?\nResponse: True:\n\nQuery: What is LangChain and how is it used in AI?\nResponse: True:\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\n# Directory names\nmodel_directories = [\"fine_tuned_model\", \"fine_tuned_model_t5\", \"fine_tuned_model_flan_t5\"]\n\n# Print last modified time of each model directory\nfor directory in model_directories:\n    timestamp = os.path.getmtime(directory)\n    last_modified = datetime.fromtimestamp(timestamp)\n    print(f\"{directory} last modified: {last_modified}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T08:15:59.794845Z","iopub.execute_input":"2024-11-03T08:15:59.795559Z","iopub.status.idle":"2024-11-03T08:15:59.801339Z","shell.execute_reply.started":"2024-11-03T08:15:59.795517Z","shell.execute_reply":"2024-11-03T08:15:59.800468Z"}},"outputs":[{"name":"stdout","text":"fine_tuned_model last modified: 2024-11-03 07:48:43.362411\nfine_tuned_model_t5 last modified: 2024-11-03 07:53:19.966198\nfine_tuned_model_flan_t5 last modified: 2024-11-03 08:08:37.509880\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import shutil\n\n# Zip the latest model directory\nshutil.make_archive(\"fine_tuned_model_flan_t5\", 'zip', \"fine_tuned_model_flan_t5\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T08:16:27.468313Z","iopub.execute_input":"2024-11-03T08:16:27.469020Z","iopub.status.idle":"2024-11-03T08:17:23.163625Z","shell.execute_reply.started":"2024-11-03T08:16:27.468980Z","shell.execute_reply":"2024-11-03T08:17:23.162699Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/fine_tuned_model_flan_t5.zip'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}